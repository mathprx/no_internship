{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from climsim_utils.data_utils import *\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = '/work/FAC/FGSE/IDYST/tbeucler/ai4pex/Physics_no_offline/ClimSim_offline/modules/climsim3/grid_info/ClimSim_low-res_grid-info.nc'\n",
    "norm_path = '/work/FAC/FGSE/IDYST/tbeucler/ai4pex/Physics_no_offline/ClimSim_offline/modules/climsim3/preprocessing/normalizations/'\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean_v2_rh_mc_pervar.nc')\n",
    "input_max = xr.open_dataset(norm_path + 'inputs/input_max_v2_rh_mc_pervar.nc')\n",
    "input_min = xr.open_dataset(norm_path + 'inputs/input_min_v2_rh_mc_pervar.nc')\n",
    "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale_std_lowerthred_v2_rh_mc.nc')\n",
    "\n",
    "\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale,\n",
    "                  qinput_log = False , # added by me\n",
    "                  input_abbrev = 'mlexpand',\n",
    "                  output_abbrev = 'mlo',\n",
    "                  normalize=True,\n",
    "                  save_zarr=False,\n",
    "                  save_h5=False,\n",
    "                  save_npy=True,\n",
    "                  cpuonly=False\n",
    "                  )\n",
    "\n",
    "\n",
    "# data = data_utils(grid_info = grid_info, \n",
    "#                   input_mean = input_mean, \n",
    "#                   input_max = input_max, \n",
    "#                   input_min = input_min, \n",
    "#                   output_scale = output_scale,\n",
    "#                   qinput_log = True) # qinout_log WAS MISSING EVERYWHERE FOR SOME REASON\n",
    "\n",
    "\n",
    "# set variables to V1 subset\n",
    "data.set_to_v2_rh_vars()  # to be changed ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load validation data\n",
    "\n",
    "The .npy files shown below were created using the `create_npy_data_splits.ipynb` notebook in the preprocessing folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/work/FAC/FGSE/IDYST/tbeucler/ai4pex/container/climsim-container/storage/climsim_highres_v2_rh_mc'\n",
    "\n",
    "val_input_path = data_path + 'vals_set/val_input.npy'\n",
    "val_target_path = data_path + 'val_set/val_target.npy'\n",
    "\n",
    "data.input_val = data.load_npy_file(val_input_path)\n",
    "data.target_val = data.load_npy_file(val_target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_pressure_grid(data_split = 'val')\n",
    "data.model_names = []\n",
    "preds = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Multiple Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLR.mlr import MultiLinearRegression, train_mlr\n",
    "\n",
    "train_inputs = torch.tensor(data.input_train, dtype=torch.float32)\n",
    "train_targets = torch.tensor(data.target_train, dtype=torch.float32)\n",
    "\n",
    "mlr = train_mlr(train_inputs,train_inputs, loss_name = 'mse')\n",
    "\n",
    "\n",
    "# Multiple Linear Regression\n",
    "mlr = MultiLinearRegression(124, 128)\n",
    "mlr.load_state_dict(torch.load('fs_mlr.pth'))\n",
    "\n",
    "X_val = torch.tensor(data.input_val, dtype=torch.float32)\n",
    "mlr.eval()\n",
    "with torch.no_grad():  \n",
    "    mlr_pred_val = mlp(X_val).numpy()\n",
    "\n",
    "print(mlr_pred_val.shape)\n",
    "data.model_names.append('mlr')\n",
    "preds.append(mlr_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pressure grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP.mlp import MLP , train_mlp\n",
    "\n",
    "\n",
    "hidden_layers = [384, 1024, 640]\n",
    "\n",
    "mlp = MLP(124, hidden_layers , 128)\n",
    "print(data.input_train.shape,data.target_train.shape)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "train_inputs = torch.tensor(data.input_train, dtype=torch.float32)\n",
    "train_targets = torch.tensor(data.target_train, dtype=torch.float32)\n",
    "\n",
    "train_mlp(mlp, train_inputs , train_targets, criterion, optimizer, epochs=20, batch_size=1024)\n",
    "torch.save(mlp.state_dict(), 'fs_mlp.pth')\n",
    "\n",
    "\n",
    "# Multiple layer perceptron\n",
    "\n",
    "mlp = MLP(124, hidden_layers , 128)\n",
    "mlp.load_state_dict(torch.load('fs_mlp.pth'))\n",
    "\n",
    "X_val = torch.tensor(data.input_val, dtype=torch.float32)\n",
    "mlp.eval()\n",
    "with torch.no_grad():  \n",
    "    mlp_pred_val = mlp(X_val).numpy()\n",
    "\n",
    "print(mlp_pred_val.shape)\n",
    "data.model_names.append('mlp')\n",
    "preds.append(mlp_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.preds_val = dict(zip(data.model_names, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight predictions and target (just for V1 ?)\n",
    "\n",
    "1. Undo output scaling\n",
    "\n",
    "2.  Weight vertical levels by dp/g\n",
    "\n",
    "3. Weight horizontal area of each grid cell by a[x]/mean(a[x])\n",
    "\n",
    "4. Convert units to a common energy unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reweight_target(data_split = 'val')\n",
    "data.reweight_preds(data_split = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.metrics_names = ['MAE', 'RMSE', 'R2', 'bias']\n",
    "data.create_metrics_df(data_split = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "# create custom dictionary for plotting\n",
    "dict_var = data.metrics_var_val\n",
    "plot_df_byvar = {}\n",
    "for metric in data.metrics_names:\n",
    "    plot_df_byvar[metric] = pd.DataFrame([dict_var[model][metric] for model in data.model_names],\n",
    "                                               index=data.model_names)\n",
    "    plot_df_byvar[metric] = plot_df_byvar[metric].rename(columns = data.var_short_names).transpose()\n",
    "\n",
    "# plot figure\n",
    "fig, axes = plt.subplots(nrows  = len(data.metrics_names), sharex = True)\n",
    "for i in range(len(data.metrics_names)):\n",
    "    plot_df_byvar[data.metrics_names[i]].plot.bar(\n",
    "        legend = False,\n",
    "        ax = axes[i])\n",
    "    print(data.metrics_names[i],plot_df_byvar[data.metrics_names[i]])\n",
    "    if data.metrics_names[i] != 'R2':\n",
    "        axes[i].set_ylabel('$W/m^2$')\n",
    "    else:\n",
    "        axes[i].set_ylim(0,1)\n",
    "\n",
    "    axes[i].set_title(f'({letters[i]}) {data.metrics_names[i]}')\n",
    "axes[i].set_xlabel('Output variable')\n",
    "axes[i].set_xticklabels(plot_df_byvar[data.metrics_names[i]].index, \\\n",
    "    rotation=0, ha='center')\n",
    "\n",
    "axes[0].legend(columnspacing = .9, \n",
    "               labelspacing = .3,\n",
    "               handleheight = .07,\n",
    "               handlelength = 1.5,\n",
    "               handletextpad = .2,\n",
    "               borderpad = .2,\n",
    "               ncol = 3,\n",
    "               loc = 'upper right')\n",
    "fig.set_size_inches(7,8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained models with different hyperparameters, use the ones that performed the best on validation data for evaluation on scoring data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on scoring data\n",
    "\n",
    "#### Do this at the VERY END (when you have finished tuned the hyperparameters for your  model and are seeking a final evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load scoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_input_path = \"/work/FAC/FGSE/IDYST/tbeucler/ai4pex/Physics_no_offline/First_step/fs_data/test_input.npy\"\n",
    "scoring_target_path = \"/work/FAC/FGSE/IDYST/tbeucler/ai4pex/Physics_no_offline/First_step/fs_data/test_target.npy\"\n",
    "# path to target input\n",
    "data.input_scoring = np.load(scoring_input_path)\n",
    "\n",
    "# path to target output\n",
    "data.target_scoring = np.load(scoring_target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pressure grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_pressure_grid(data_split = 'scoring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant prediction\n",
    "const_pred_scoring = np.repeat(const_model[np.newaxis, :], data.target_scoring.shape[0], axis = 0)\n",
    "print(const_pred_scoring.shape)\n",
    "\n",
    "# multiple linear regression\n",
    "X_scoring = data.input_scoring\n",
    "bias_vector_scoring = np.ones((X_scoring.shape[0], 1))\n",
    "X_scoring = np.concatenate((X_scoring, bias_vector_scoring), axis=1)\n",
    "mlr_pred_scoring = X_scoring@mlr_weights\n",
    "print(mlr_pred_scoring.shape)\n",
    "\n",
    "# Your model prediction here\n",
    "\n",
    "# Load predictions into object\n",
    "data.model_names = ['const', 'mlr'] # model name here\n",
    "preds = [const_pred_scoring, mlr_pred_scoring] # add prediction here\n",
    "data.preds_scoring = dict(zip(data.model_names, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight predictions and target\n",
    "\n",
    "1. Undo output scaling\n",
    "\n",
    "2.  Weight vertical levels by dp/g\n",
    "\n",
    "3. Weight horizontal area of each grid cell by a[x]/mean(a[x])\n",
    "\n",
    "4. Convert units to a common energy unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight predictions and target\n",
    "data.reweight_target(data_split = 'scoring')\n",
    "data.reweight_preds(data_split = 'scoring')\n",
    "\n",
    "# set and calculate metrics\n",
    "data.metrics_names = ['MAE', 'RMSE', 'R2', 'bias']\n",
    "data.create_metrics_df(data_split = 'scoring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "# create custom dictionary for plotting\n",
    "dict_var = data.metrics_var_scoring\n",
    "plot_df_byvar = {}\n",
    "for metric in data.metrics_names:\n",
    "    plot_df_byvar[metric] = pd.DataFrame([dict_var[model][metric] for model in data.model_names],\n",
    "                                               index=data.model_names)\n",
    "    plot_df_byvar[metric] = plot_df_byvar[metric].rename(columns = data.var_short_names).transpose()\n",
    "\n",
    "# plot figure\n",
    "fig, axes = plt.subplots(nrows  = len(data.metrics_names), sharex = True)\n",
    "for i in range(len(data.metrics_names)):\n",
    "    plot_df_byvar[data.metrics_names[i]].plot.bar(\n",
    "        legend = False,\n",
    "        ax = axes[i])\n",
    "    if data.metrics_names[i] != 'R2':\n",
    "        axes[i].set_ylabel('$W/m^2$')\n",
    "    else:\n",
    "        axes[i].set_ylim(0,1)\n",
    "\n",
    "    axes[i].set_title(f'({letters[i]}) {data.metrics_names[i]}')\n",
    "axes[i].set_xlabel('Output variable')\n",
    "axes[i].set_xticklabels(plot_df_byvar[data.metrics_names[i]].index, \\\n",
    "    rotation=0, ha='center')\n",
    "\n",
    "axes[0].legend(columnspacing = .9, \n",
    "               labelspacing = .3,\n",
    "               handleheight = .07,\n",
    "               handlelength = 1.5,\n",
    "               handletextpad = .2,\n",
    "               borderpad = .2,\n",
    "               ncol = 3,\n",
    "               loc = 'upper right')\n",
    "fig.set_size_inches(7,8)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
